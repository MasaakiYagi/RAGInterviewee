{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eufo3\\miniconda3\\envs\\openai_webapp\\Lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import io\n",
    "import time\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "# .envファイルからAPIキーとアシスタントIDを読み込む\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "assistant_id = os.getenv('ASSISTANT_ID')\n",
    "\n",
    "# # クライアント設定\n",
    "# client = OpenAI()\n",
    "# client.api_key = api_key\n",
    "\n",
    "# STT, Assistants, TTSのパイプラインクラス\n",
    "class AIAssistant:\n",
    "    \"\"\"\n",
    "    OpenAIのAPIを利用して音声をテキストに変換し、AIアシスタントで処理し、音声に戻すクラス。\n",
    "    \"\"\"\n",
    "\n",
    "    # 録音パラメータ\n",
    "    fs = 44100  # サンプリングレート\n",
    "    duration = 5  # 録音する秒数\n",
    "    channels = 1  # モノラル録音\n",
    "    # 音声認識モデル\n",
    "    stt_model = \"whisper-1\"\n",
    "    # 音声生成モデル\n",
    "    tts_model = \"tts-1\"  # 高品質モデル tts-1-hd\n",
    "    # 声質\n",
    "    voice_code = \"nova\"  # 男性 alloy, echo, fable, onyx 女性 nova, shimmer\n",
    "\n",
    "    def __init__(self, assistant_id: str, api_key: str, output_audio_file: str = \"./output.wav\"):\n",
    "        \"\"\"\n",
    "        初期化処理。\n",
    "\n",
    "        Args:\n",
    "            assistant_id (str): AIアシスタントのID。\n",
    "            output_audio_file (str, optional): 音声ファイルの保存先。\n",
    "        \"\"\"\n",
    "        self.assistant_id = assistant_id\n",
    "        self.client = OpenAI()\n",
    "        self.client.api_key = api_key\n",
    "        thread = self.client.beta.threads.create()\n",
    "        self.thread_id = thread.id\n",
    "        self.output_audio_file = output_audio_file\n",
    "\n",
    "    def record_audio(self) -> any:\n",
    "        \"\"\"\n",
    "        オーディオを録音する。\n",
    "\n",
    "        Returns:\n",
    "            any: 録音データ。\n",
    "        \"\"\"\n",
    "        print(\"Start recording...\")\n",
    "        recorded_data = sd.rec(\n",
    "            int(self.duration * self.fs), samplerate=self.fs, channels=self.channels\n",
    "        )\n",
    "        sd.wait()  # 録音が終わるまで待機\n",
    "        print(\"...Finished recording\")\n",
    "        return recorded_data\n",
    "\n",
    "    def transcribe_audio(self, audio_data: any) -> str:\n",
    "        \"\"\"\n",
    "        録音されたオーディオをテキストに変換する。\n",
    "\n",
    "        Args:\n",
    "            audio_data (any): 録音データ。\n",
    "\n",
    "        Returns:\n",
    "            str: 変換されたテキスト。\n",
    "        \"\"\"\n",
    "        write(self.output_audio_file, self.fs, audio_data)\n",
    "        with open(self.output_audio_file, \"rb\") as audio_file:\n",
    "            transcript = self.client.audio.transcriptions.create(\n",
    "                model=self.stt_model, file=audio_file\n",
    "            )\n",
    "        return transcript.text\n",
    "\n",
    "    def run_thread_actions(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        テキストをAIアシスタントに送信し、応答を取得する。\n",
    "\n",
    "        Args:\n",
    "            text (str): ユーザーからのテキスト。\n",
    "\n",
    "        Returns:\n",
    "            str: アシスタントからの応答テキスト。\n",
    "        \"\"\"\n",
    "        self.client.beta.threads.messages.create(\n",
    "            thread_id=self.thread_id,\n",
    "            role=\"user\",\n",
    "            content=text,\n",
    "        )\n",
    "\n",
    "        run = self.client.beta.threads.runs.create(\n",
    "            thread_id=self.thread_id,\n",
    "            assistant_id=self.assistant_id,\n",
    "        )\n",
    "\n",
    "        while True:\n",
    "            result = self.client.beta.threads.runs.retrieve(\n",
    "                thread_id=self.thread_id, run_id=run.id\n",
    "            )\n",
    "            if result.status == \"completed\":\n",
    "                break\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        messages = self.client.beta.threads.messages.list(\n",
    "            thread_id=self.thread_id, order=\"asc\"\n",
    "        )\n",
    "\n",
    "        if len(messages.data) < 2:\n",
    "            return \"\"\n",
    "\n",
    "        return messages.data[-1].content[0].text.value\n",
    "\n",
    "    def text_to_speech(self, text: str) -> None:\n",
    "        \"\"\"\n",
    "        テキストを音声に変換し、再生する。\n",
    "\n",
    "        Args:\n",
    "            text (str): 再生するテキスト。\n",
    "        \"\"\"\n",
    "        response = self.client.audio.speech.create(\n",
    "            model=self.tts_model,\n",
    "            voice=self.voice_code,\n",
    "            input=text,\n",
    "        )\n",
    "\n",
    "        byte_stream = io.BytesIO(response.content)\n",
    "\n",
    "        audio = AudioSegment.from_file(byte_stream, format=\"mp3\")\n",
    "\n",
    "        play(audio)\n",
    "\n",
    "def main():\n",
    "    ai_assistant = AIAssistant(assistant_id=assistant_id)\n",
    "\n",
    "    while True:\n",
    "        recorded_data = ai_assistant.record_audio()\n",
    "        transcript_text = ai_assistant.transcribe_audio(recorded_data)\n",
    "        print(f\"user: {transcript_text}\")\n",
    "\n",
    "        if not transcript_text:\n",
    "            break\n",
    "\n",
    "        assistant_content = ai_assistant.run_thread_actions(transcript_text)\n",
    "        print(f\"assistant: {assistant_content}\")\n",
    "        ai_assistant.text_to_speech(assistant_content)\n",
    "\n",
    "# # OpenAI APIキーを設定\n",
    "# openai.api_key = api_key\n",
    "\n",
    "# def get_assistant_response(user_input):\n",
    "#     response = openai.Completion.create(\n",
    "#         engine=\"davinci-codex\",  # 使用するエンジンを指定\n",
    "#         prompt=f\"{assistant_id}: {user_input}\",\n",
    "#         max_tokens=150,\n",
    "#         n=1,\n",
    "#         stop=None,\n",
    "#         temperature=0.9\n",
    "#     )\n",
    "#     return response.choices[0].text.strip()\n",
    "\n",
    "# def main():\n",
    "#     print(\"対話を開始します。終了するには 'exit' と入力してください。\")\n",
    "#     while True:\n",
    "#         user_input = input(\"あなた: \")\n",
    "#         if user_input.lower() == 'exit':\n",
    "#             break\n",
    "#         assistant_response = get_assistant_response(user_input)\n",
    "#         print(f\"アシスタント: {assistant_response}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_webapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
